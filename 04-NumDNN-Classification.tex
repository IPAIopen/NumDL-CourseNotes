\documentclass[12pt,fleqn,handout]{beamer}

\input{beamerStyle.tex}
\input{abbrv.tex}


\title{Classification}
\subtitle{Numerical Methods for Deep Learning}
\date{}
\begin{document}

\makebeamertitle

\section{Logistic Regression and Softmax} % (fold)
\label{sec:logistic_regression_and_softmax}

\begin{frame}
	\frametitle{Logistic Regression}
	
	
	Assume our data falls into two classes. Denote by $\bfc_{\rm obs}(\bfy)$ the probability that example $\bfy\in\R^{n_f}$ belongs to first category.
	
	\bigskip
	
	Since output of our classifier $f(\bfy,\bftheta)$ is supposed to be probability, use logistic sigmoid
	$$
		\bfc(\bfy,\bftheta) = \frac{1}{1 + \exp\left(-f(\bfy,\bftheta)\right)}.
	$$
	
	\bigskip
	
	Example (Linear Classification): If $f(\bfy,\bftheta)$ is a linear function (adding bias is easy), $\bftheta = \bfw \in \R^{n_f}$ and
	$$
		\bfc(\bfy,\bfw) = \frac{1}{1 + \exp(-\bfw^\top\bfy)}.
	$$
	
	\begin{center}
		for now: consider linear models for simplicity
	\end{center}
	
\end{frame}

\begin{frame}
	\frametitle{Multinomial Logistic Regression}
	
	Suppose data falls into $n_c\geq 2$ categories and the components of $\bfc_{\rm obs}(\bfy) \in[0,1]^{n_c}$ contain probabilities for each class. 
	
	\bigskip
	 
	Applying the logistic sigmoid to each component of $f(\bfy,\bfW)$ not enough (probabilities must sum to one). Use
	$$
		\bfc(\bfy,\bfW) = \left( \frac{1}{\bfe_{n_c}^\top\exp(\bfW\bfy)} \right) \; \exp(\bfW\bfy).
	$$
	
	\bigskip
	
	Note: Division and $\exp$ are applied element-wise!
	
\end{frame}
\begin{frame}
	\frametitle{Logistic Regression - Loss Function}
	
	How similar are $\bfc(\cdot,\bfW)$ and $\bfc_{\rm obs}(\cdot)$?

	\begin{columns}
		\column{.7\textwidth}
	Naive idea: Let $\bfY \in \R^{n_f\times n}$ be examples with class probabilities $\bfC_{\rm obs} \in [0,1]^{n_c\times n}$, use 
	$$
	\phi(\bfW)= \frac{1}{2n} \sum_{j=1}^n\|\bfc(\bfy_j,\bfW) - \bfc_{j,{\rm obs}} \|_F^2
	$$

Problems
\begin{itemize}
	\item ignores that $\bfc(\cdot,\bfW)$ and $\bfc_{\rm obs}(\cdot)$ are distributions.
	\item leads to non-convex objective function
\end{itemize} 
		
		\column{.4\textwidth}
		
		\begin{tabular}{c}
			Frobenius\\
			\includegraphics[width=\textwidth]{Class_Frobenius}\\
			Cross Entropy\\
			\includegraphics[width=\textwidth]{Class_CrossEntropy}
		\end{tabular}
	\end{columns}
	
\end{frame}


\begin{frame}
	\frametitle{Example: Designing a Code}
	
	Goal: Communicate using minimal number of bits.
	
	\bigskip
	\pause
	
	\invisible<beamer|1>{
	Example: Bob talks $\bfc = [1/2,1/4,1/8,1/8]$ of the time about dogs, cats, fish, and birds, respectively.
	
	How many bits need to be transferred on average?}

	\bigskip
	
	\invisible<beamer|-2>{
	\begin{center}
		\begin{tabular}{l|cc}
			word & naive code & \invisible<beamer|-3>{better code} \\ \hline
			dog  & 00         & \invisible<beamer|-3>{ 0 }         \\ 
			cat  & 01         & \invisible<beamer|-3>{ 10   }      \\
			fish & 10         & \invisible<beamer|-3>{ 110  }      \\ 
			bird & 11         & \invisible<beamer|-3>{ 111   }     \\
		\end{tabular}		
	\end{center}
	}
	
	\invisible<beamer|-4>{
	\begin{center}
		Idea: Quantify information content in probability distribution using average length.		
	\end{center}
	}
	
	
	\only<beamer|5>{}
	

\end{frame}

\begin{frame}
	\frametitle{Entropy}

	Note: Length of word depends on its probability being used. How long should a word be?
	
	\bigskip
	\pause
	
	Optimal choice for information for any category
$$ I = \log_2(\bfc_j^{-1}) = -\log_2(\bfc_j) $$


\bigskip

The larger $\bfc_j$, the more common we use it, the shorter the word should be. 

\bigskip
\pause

The entropy is the average (expectation) of information over all classes.

$$ E(\bfc) = -\sum \bfc_j \log_2(\bfc_j) = -\bfc^{\top} \log_2(\bfc) $$

\end{frame}

\begin{frame}
	\frametitle{Example: Designing a Code - 2}

	Entropy for Bob's code is
	$$
		\frac{1}{2} \log\left(2\right) + \frac{1}{4} \log\left(4\right) + 2 \frac{1}{8} \log\left(8\right) = 1.75
	$$
	average length of word is 1.75 bits $<$ 2 bits for naive code!
	
\bigskip


\bigskip
\pause

For the complete tutorial on entropy, read
{\small
\tt{http://colah.github.io/posts/2015-09-Visual-Information/}
}

\end{frame}

\begin{frame}
	\frametitle{Properties of Entropy}

\begin{itemize}
\item recall $\lim_{x\rightarrow 0} x \log x = 0$
\item prefer sparse distributions (why?)
\item has been used in compressed sensing type methods
\end{itemize}

\begin{center}
\begin{figure}
\includegraphics[width=5cm]{entropy.jpg}
\caption{The entropy of a vector $\bfc = (c_1, c_2)^\top$}
\end{figure}
\end{center}


\end{frame}

\begin{frame}
	\frametitle{Cross Entropy}


Measure the average word length when using code designed for $\bfc$ for sending information with probability $\widehat \bfc$
$$ E(\widehat\bfc, \bfc) = - {\widehat \bfc}^{\top} \log(\bfc). $$

\bigskip

Clearly
$$ E(\widehat\bfc,  \bfc) \ge E(\bfc, \bfc) $$

\bigskip
\pause

Example: Alice talks $\bfc = [1/8,1/2,1/4,1/8]$ of the time about dogs, cats, fish, and birds, respectively.
If she used Bob's code, the average word length would be 
$$
	\frac{1}{8} \log\left(2\right) + \frac{1}{2} \log\left(4\right) + \frac{1}{4} \log\left(8\right) + \frac{1}{8} \log\left(8\right) = 2.25 > 1.75
$$

\bigskip
\pause

$E$ measures how similar the distributions $\bfc$ and $\widehat \bfc$ are.


\bigskip
\pause

One flaw: $ E(\bfc, \widehat \bfc) \not= E(\widehat \bfc, \bfc)$ (verify for our example!)


\end{frame}

\begin{frame}
	\frametitle{Cross Entropy for Logistic Regression - 1}

Recall: For a single example and two classes we have
 $$
  \bfc(\bfy,\bfw)  = \begin{pmatrix}
  	{\frac 1 {1+\exp(-\bfw^{\top} \bfy)}} \\
	1-{\frac 1 {1+\exp(-\bfw^{\top} \bfy)}}
  \end{pmatrix} 
  \stackrel{\rm def}{=} 
  \begin{pmatrix}
  	h(\bfw^{\top}\bfy) \\1-h(\bfw^{\top}\bfy)
  \end{pmatrix}
 $$

Assume we have the observation $\bfc_{\rm obs} = \begin{pmatrix}
	\bfc_{\rm obs} \\ 1-\bfc_{\rm obs}
\end{pmatrix}$ then
\begin{align*}
 E(\bfc_{\rm obs}, \bfc) & = -\bfc_{\rm obs}^\top\log(\bfc(\bfy,\bfw)) \\
 & = -\bfc_{\rm obs} \log(h(\bfw^{\top}\bfy)) - (1-\bfc_{\rm obs})
\log(1-h(\bfw^{\top}\bfy)). 
\end{align*}
where
$$
 h(z) = \frac{1}{1+\exp(-z)}
$$
\end{frame}

\begin{frame}
	\frametitle{Cross Entropy for Logistic Regression - 2}


In the case we have many examples need to sum over the data
\begin{align*}
	\bfC(\bfY,\bfw) & = \begin{pmatrix}
		{\frac 1 {1+\exp(-\bfw^\top \bfY)}} \\
		1-{\frac 1 {1+\exp(\bfw^\top\bfY)}}
	\end{pmatrix}  \stackrel{def}{=} \begin{pmatrix}
		h(\bfw^\top \bfY)\\ 1-h(\bfw^\top\bfY)
	\end{pmatrix}
	\in \R^{2\times n}
\end{align*}
Assume we have the observation $\bfc_{\rm obs}\in\R^n$. Define
$$
\bfC_{\rm obs} = \begin{pmatrix}
	\bfc_{\rm obs}^\top \\ 1-\bfc_{\rm obs}^\top
\end{pmatrix} \in \R^{2 \times n}.
$$

Then the cross entropy is
\begin{align*}
	E(\bfC_{\rm obs}, \bfC)  =& - \frac{1}{n} {\rm tr}(\bfC_{\rm obs}^\top \bfC) \\
	 						 = & -\frac{1}{n} \bfc_{\rm obs}^{\top} \log(h(\bfw^\top\bfY) \\
							   & - \frac{1}{n}(1-\bfc_{\rm obs})^{\top}\log(1-h(\bfw^\top\bfY)))
\end{align*}

\end{frame}

\begin{frame}
	\frametitle{Cross Entropy for Multinomial Logistic Regression}
	
	Similarly, for general case ($n_c\geq2$ classes, $n$ examples). 
	
	Recall: 
	$$\bfC(\bfY,\bfW)  =   \exp(\bfW \bfY)\; {\rm diag} \left({\frac 1 {\bfe_{n_c}^\top\exp(\bfW \bfY)}}  \right)  $$

\bigskip

Get cross entropy by summing over all examples

$$ E(\bfC_{\rm obs},\bfC(\bfY,\bfW)) = -\frac{1}{n}{\rm tr}( \bfC_{\rm obs}^\top\log(\bfC(\bfY,\bfW)) ). $$

We will also call this the \emph{softmax} (cross-entropy) function.


\end{frame}


\begin{frame}\frametitle{Simplifying the Softmax Function}

Let $\bfS = \bfW \bfY$, then 
$$ 
E(\bfC_{\rm obs},\bfS) = -\frac{1}{n}{\rm tr}\left(\bfC_{\rm obs}^{\top} \log\left({\exp(\bfS)}{\rm diag}\left(\frac{1}{\bfe_{n_c}^\top \exp(\bfS) }\right) \right)\right). 
$$
\pause

Verify that this is equal to
\begin{align*}
	 E(\bfC_{\rm obs},\bfS) = & -\frac{1}{n}\bfe_{n_c}^{\top} \left( \bfC_{\rm obs} \odot \bfS\right) \bfe_{n} \\
	 & + \frac{1}{n} \bfe_{n_c}^\top \bfC_{\rm obs}\log\left(\bfe_{n_c}^\top\exp(\bfS)\right)^\top
\end{align*}
($\odot$ is Hadamard product, $\exp$ and $\log$ component-wise)
\bigskip
\pause


If $\bfC_{\rm obs}$ has a unit row sum (why?) then $ \bfe_{n_c}^\top \bfC_{\rm obs}^\top = \bfe_n^\top$
and 
$$ E(\bfC_{\rm obs},\bfS) = -\frac{1}{n}\bfe_{n_c}^{\top} \left( \bfC_{\rm obs} \odot \bfS\right) \bfe_{n} 
+ \frac{1}{n}\log(\bfe_{n_c}^\top\exp(\bfS))\bfe_{n} $$


\end{frame}



\begin{frame}\frametitle{Numerical Considerations}

Scale to prevent overflow. Note that for an arbitrary $s\in\R$ we have
$$ E(\bfC_{\rm obs},\bfW\bfY-s) = E(\bfC_{\rm obs},\bfW\bfY) $$

This prevents overflow, but may lead to underflow (and divisions by zero).

\bigskip
\pause

Note that $s$ does not need to be the same in each row (example). Hence, 
we can choose $\bfs = \max(\bfW\bfY,[],1 ) \in \R^{1\times n}$ to avoid underflow and overflow.

\bigskip
\begin{center}
For stability use $E(\bfC_{\rm obs},\bfS)$ where $ \bfS = \bfW \bfY - \bfe_{n_c} \bfs$.
\end{center}
\end{frame}

\begin{frame}[fragile]\frametitle{Test Problem: Linear Classification}

Generate data that is linearly separable:

\begin{center}
	\includegraphics[height=40mm]{linearClass}
\end{center}
\vspace*{-5mm}
\begin{verbatim}

a = 3; b = 2;

Y = randn(2,500);
C = a*Y(1,:) + b*Y(2,:) + 1;
C(C>0) = 1; C(C<0) = 0;
C = [C; 1-C]

\end{verbatim}


\end{frame}


\begin{frame}[fragile]\frametitle{Coding: Softmax Regression Objective Function}


Write a function that computes the softmax function given a data matrix $\bfY$,
its class $\bfC$, and a matrix $\bfW$.

\bigskip

\begin{verbatim}
function[E] = softmaxFun(W,Y,C)

% Your code here


end
\end{verbatim}
\begin{center}
	Test your code using \texttt{testSoftmax.m}
\end{center}
\end{frame}


\begin{frame}\frametitle{Linear Classification}

If $\bfW$ can separate the classes then the goal is to minimize the cross entropy (with some potential regularization)

\begin{align*}
 \bfW^* = & \argmin_{\bfW} \quad - \frac{1}{n}\bfe_{n_c}^{\top} \left( \bfC_{\rm obs} \odot \bfS\right) \bfe_{n} 
+ \frac{1}{n} \log(\bfe_{n_c}^\top \exp(\bfS))\bfe_n \\
          & \text{subject to} \;\bfS = \bfW \bfY - \bfe_{n_c} \bfs
\end{align*}
\pause

This is a smooth convex optimization problem \\ $\Rightarrow$ many existing optimization techniques will work

\bigskip

For large-scale problems, use derivative-based optimization algorithm. (Examples: Steepest Descent, Newton-like methods,  Stochastic Gradient Descent, ADMM, \ldots)

\bigskip

Excellent references: \cite{NocedalWright2006,BoydVandenberghe2004,Beck2014,WuFungEtAl2019SoftmaxADMM}
\end{frame}

 \begin{frame}\frametitle{Differentiating the Softmax Function - 1}

We need to compute the derivative of the cross entropy function with respect to $\bfW$.
Three hints:
\begin{itemize}
\item $\sum \bfw \odot \bfy = \bfw^{\top} \bfy $
\item $\grad_{\bfw} (\bfw^{\top} \bfy) = \bfy$
\item ${\rm vec} (\bfW \bfY) = (\bfY^\top \otimes \bfI) {\rm vec} (\bfW) = (\bfI \otimes \bfW) {\rm vec}(\bfY)$
\end{itemize}
where $\otimes$ is the Kronecker product.

\bigskip



We use the common notation:
\begin{itemize}
	\item ${\rm vec}(\bfA)$ reshapes matrix $\bfA$ (column-wise) into vector
	\item ${\rm mat}(\bfa)$ reshapes vector $\bfa$ into matrix, ${\rm mat}({\rm vec}(\bfA)) = \bfA$.
	\item ${\rm diag}(\bfA) = {\rm diag}({\rm vec}( \bfA))$ builds diagonal matrix with entries given by $\bfA$. 
\end{itemize}


\end{frame}

\begin{frame}[fragile]\frametitle{Differentiating the Softmax Function - 2}

Do it in three steps:
\begin{enumerate}
	\item $\nabla_{\bfS} E(\bfS)$ with $\bfS = \bfY \bfW$ (assume w.l.o.g. no shift)
	\item $\nabla_{\bfW} \bfS = \nabla_{\bfW} (\bfW \bfY)$
	\item use chain rule to get $\nabla_{\bfW} E(\bfW\bfY)$.
\end{enumerate}

\bigskip
\pause

Break the first step down into two terms
$$ E(\bfS) = \frac{1}{n} \overbrace{-{\rm tr}(\bfC_{\rm obs}^{\top} \bfS)}^{E1} + \frac{1}{n} \overbrace{\log(\bfe_{n_c}\exp(\bfS))\bfe_n}^{E2}, $$


\bigskip
\pause

First term is linear

$$\grad_{\bfS}E_1 =  \grad_{\bfS} {\rm tr}(\bfC_{\rm obs}^{\top} \bfS) = \bfC_{\rm obs}. $$

\end{frame}

\begin{frame}[fragile]\frametitle{Differentiating the Softmax Function - 3}


$$ E(\bfS) = \frac{1}{n}\overbrace{-{\rm tr}(\bfC_{\rm obs}^{\top} \bfS)}^{E1} + \frac{1}{n} \overbrace{\log(\bfe_{n_c}^\top\exp(\bfS))\bfe_n}^{E2}  $$


\bigskip

Second  term requires a bit more care

$$ \bfJ_{\bfS}E_2 = \bfJ_{\bfS} \bfe_n^{\top}\log(\bfe_{n_c}^\top \exp(\bfS)) = 
 \bfe_n^{\top}\bfJ_{\bfS}  \log(\bfe_{n_c}^\top\exp(\bfS))  $$

\pause 
 and
 
$$ \bfJ_{\bfS}  \log(\bfe_{n_c}^\top\exp(\bfS)) = {\rm diag}
\left( {\frac {1}{\bfe_{n_c}^\top\exp(\bfS)}}\right)
\bfJ_{\bfS}\left(\bfe_{n_c}^\top\exp(\bfS) \right) $$
 

\end{frame}


\begin{frame}[fragile]\frametitle{Differentiating the Softmax Function - 4}
	Recall:
$$ \bfJ_{\bfS}  E_2 = \bfe_n^\top  {\rm diag}
\left( {\frac {1}{\bfe_{n_c}^\top\exp(\bfS)}}\right)
\bfJ_{\bfS}\left(\bfe_{n_c}^\top\exp(\bfS) \right) $$

Focus on last term:
\begin{align*}
   \bfJ_{\bfS} (\bfe_{n_c}^\top \exp(\bfS)) & = \bfJ_{\bfS} \left(\left(\bfI \otimes \bfe_{n_c}^{\top} \right)  {\rm vec}(\exp(\bfS))\right)\\
   &  = (\bfI \otimes \bfe_{n_c}^{\top})  {\rm diag}( {\rm vec}(\exp(\bfS)))
\end{align*}
 
 \pause
 Putting it together
 
 
 $$ \bfJ_{\bfS}E_2 =  \bfe_n^{\top}\ {\rm diag}
\left( {\frac {1}{\bfe_{n_c}^\top \exp(\bfS)}}\right) \  ( \bfI \otimes \bfe_{n_c}^{\top})  {\rm diag}( {\rm vec}(\exp(\bfS))) $$


\pause

\begin{center}
	Left to do: Take transpose
\end{center}


\end{frame}

\begin{frame}[fragile]\frametitle{Differentiating the Softmax Function - 5}

 
 $$ \grad_{\bfS}E_2 =     {\rm diag}( {\rm vec}(\exp(\bfS))) (\bfI \otimes \bfe_{n_c})
  {\rm diag}
\left( {\frac {1}{\bfe_{n_c}^\top\exp(\bfS)}}\right) \bfe_n
     $$
\pause
simplifying

 $$ \grad_{\bfS}E_2 =     {\rm diag}( {\rm vec}(\exp(\bfS))) (\bfI \otimes \bfe_{n_c})
\left( {\frac {1}{\bfe_{n_c}^\top\exp(\bfS)}}\right)
     $$
 
\pause    
     Avoid ${\rm diag}$ and simplify using  matrix representation
     
 $$ \grad_{\bfS}E_2 =     \exp(\bfS) \odot 
\left( \bfe_{n_c} \left({\frac {1}{\bfe_{n_c}^\top \exp(\bfS)}}\right)\right) 
     $$
     
     \bigskip
     \pause
     Finally combine gradients of $E_1$ and $E_2$
     
$$     \grad_{\bfS}E =  -\frac{1}{n}\bfC_{\rm obs} + \frac{1}{n}\exp(\bfS) \odot 
\left( \bfe_{n_c} \left({\frac {1}{\bfe_{n_c}^\top \exp(\bfS)}}\right)\right) . $$
     
    

\end{frame}

\begin{frame}[fragile]\frametitle{Differentiating the Softmax Function - 6}

$$ E(\bfW) = -\frac{1}{n}\overbrace{{\rm tr}(\bfC_{\rm obs}^{\top} (\bfW\bfY))}^{E1} + \frac{1}{n} \overbrace{\bfe_n^{\top}\log(\bfe_{n_c}^\top \exp(\bfW\bfY))}^{E2}  $$

\bigskip
Note that
$$
	\nabla_{\bfW} \bfS = \nabla_{\bfW} (\bfW \bfY) = \nabla_{\bfW} \left( (\bfY^\top \otimes \bfI) {\rm vec}(\bfW) \right) = \bfY \otimes \bfI. 
$$


\bigskip

Hence, applying the chain rule gives
     
$$     \grad_{\bfW}E = \frac{1}{n} \left( -\bfC_{\rm obs} + \exp(\bfS) \odot 
\left( \bfe_{n_c} \left({\frac {1}{\bfe_{n_c}^\top \exp(\bfS)}}\right)\right) \right)\bfY^\top. $$
     

\end{frame}


\begin{frame}[fragile]\frametitle{Coding: Differentiating the Softmax Function}

Extend your softmax function, so that it returns the gradient if needed.

\begin{verbatim}
function[E,dE] = softmaxFun(W,Y,C)

% Your code from before

if nargout > 1

% Your code for gradient here
end

end
\end{verbatim}

\end{frame}



\begin{frame}[fragile]\frametitle{Testing your Derivatives}

Your derivatives are assumed to be wrong unless you prove otherwise.

Test based on Taylor theorem. Let $\bfW$ be fixed and $\bfD$ be a random direction (same size as $\bfW$):

{\begin{small}
\begin{center}
\begin{tabular}{c|c|c}
 $h$  & $E(\bfW + h\bfD) -    E(\bfW)$ &  $E(\bfW + h\bfD) -    E(\bfW) - h {\rm tr}(\bfD^{\top} \nabla E(\bfW))$ \\
 \hline
 1   &   &   \\
 $2^{-1}$   &   &   \\
 $2^{-2}$   &   &   \\
 $2^{-3}$   &   &   \\
 $2^{-4}$   &   &   \\
 $2^{-5}$   &   &
 \end{tabular}
 \end{center}
 \end{small}}


\pause

First column should decay as ${\cal O}(h)$ \\
Second column should decay as ${\cal O}(h^2)$ \\


\end{frame}



\begin{frame}\frametitle{Derivative-Based Optimization: Steepest Descent}


To minimize the energy go ``down-hill''
\begin{center}
	\includegraphics[width=5cm]{steepestDescent.jpg}
\end{center}

Iterate:
$$ \bfW_{k+1} = \bfW_k + \alpha \bfD, \quad \bfD =  -\grad E(\bfW_k). $$
Guaranteed to be a descent direction but need to make sure that
$$ E(\bfW_k + \alpha \bfD) < E(\bfW_k)  $$

\end{frame}

\begin{frame}
	\frametitle{Line Search Problem}
	\begin{center}
		\iwidth=40mm
		\iheight=30mm
		\begin{tabular}{cc}
			objective function
			&
			linesearch
			\\
			\scriptsize
			\input{images/lineSearch-contour.tex}
			&
			\scriptsize
			\input{images/lineSearch-linesearch.tex}
		\end{tabular}
	\end{center}
	Let $E$ be the cross entropy, $\bfW_k$ the current weights, and $\bfD$ the search direction.  The line search problem is:
	\begin{equation*}
		\min_{\alpha > 0} \phi(\alpha) \quad \text{ where } \quad \phi(\alpha) = E(\bfW_k + \alpha \bfD).
	\end{equation*}
\end{frame}


\begin{frame}\frametitle{Armijo Line Search}

A method for inexact line search


\begin{itemize}
\item Start with $\alpha = \alpha_0$
\item Test $ E(\bfW + \alpha \bfD) {<} E(\bfW)$
\item If fail $\alpha \leftarrow \hf \alpha$
\end{itemize}

\bigskip
\pause

A few (small) but helpful tricks
\begin{itemize}
\item Choose your $\alpha_0$ based on the problem
\item If line search is needed ($\alpha\not=\alpha_0$) at iteration $k$ set $\alpha_0 = \alpha_k$ in next iteration.
\item If no line search is needed ($\alpha =\alpha_0$) at iteration $k$ set $\alpha_0 = \gamma\alpha_k$, $\gamma>1$ in next iteration.
\end{itemize}


\end{frame}


\begin{frame}[fragile]\frametitle{Coding: Steepest Descent }

Write a code for steepest descent with Armijo linesearch.

\begin{verbatim}
function W = steepestDescent(E,W,param)
% Inputs:
%     E  - function that provides value and gradient
%     W  - starting guess
%  param - struct with parameters

alpha     = param.maxStep; % max step size
maxIter   = param.maxIter; % max number of iterations

for i=1:maxIter

% Your code here
end
\end{verbatim}

\end{frame}



\begin{frame}[allowframebreaks]
	\frametitle{References}
\bibliographystyle{abbrv}
\bibliography{NumDNN}

\end{frame}

\end{document}
